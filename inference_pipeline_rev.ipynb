{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "boto_session = boto3.Session(region_name=\"us-east-1\")\n",
    "sagemaker_client = boto_session.client(\"sagemaker\")\n",
    "runtime_client = boto_session.client(\"sagemaker-runtime\")\n",
    "sagemaker_session = sagemaker.session.Session(\n",
    "    boto_session=boto_session,\n",
    "    sagemaker_client=sagemaker_client,\n",
    "    sagemaker_runtime_client=runtime_client,\n",
    "    # default_bucket=default_bucket,\n",
    ")\n",
    "\n",
    "role = sagemaker.session.get_execution_role(sagemaker_session)\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = \"Scikit-LinearLearner-pipeline-abalone-example\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'sagemaker-us-east-1-926521026587'"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing data and training the model <a class=\"anchor\" id=\"training\"></a>\n",
    "## Downloading dataset <a class=\"anchor\" id=\"download_data\"></a>\n",
    "SageMaker team has downloaded the dataset from UCI and uploaded to one of the S3 buckets in our account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--2021-06-17 17:20:27--  https://s3-us-west-2.amazonaws.com/sparkml-mleap/data/abalone/abalone.csv\n",
      "Resolving s3-us-west-2.amazonaws.com (s3-us-west-2.amazonaws.com)... 52.218.241.8\n",
      "Connecting to s3-us-west-2.amazonaws.com (s3-us-west-2.amazonaws.com)|52.218.241.8|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 191873 (187K) [binary/octet-stream]\n",
      "Saving to: ‘./abalone_data/abalone.csv’\n",
      "\n",
      "100%[======================================>] 191,873      780KB/s   in 0.2s   \n",
      "\n",
      "2021-06-17 17:20:27 (780 KB/s) - ‘./abalone_data/abalone.csv’ saved [191873/191873]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget --directory-prefix=./abalone_data https://s3-us-west-2.amazonaws.com/sparkml-mleap/data/abalone/abalone.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the data for training <a class=\"anchor\" id=\"upload_data\"></a>\n",
    "\n",
    "When training large models with huge amounts of data, you'll typically use big data tools, like Amazon Athena, AWS Glue, or Amazon EMR, to create your data in S3. We can use the tools provided by the SageMaker Python SDK to upload the data to a default bucket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORK_DIRECTORY = \"abalone_data\"\n",
    "\n",
    "train_input = sagemaker_session.upload_data(\n",
    "    path=f\"{WORK_DIRECTORY}/abalone.csv\",\n",
    "    bucket=bucket,\n",
    "    key_prefix=f\"{prefix}/train\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-926521026587/Scikit-LinearLearner-pipeline-abalone-example/train/abalone.csv'"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "train_input "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create SageMaker Scikit Estimator <a class=\"anchor\" id=\"create_sklearn_estimator\"></a>\n",
    "\n",
    "To run our Scikit-learn training script on SageMaker, we construct a `sagemaker.sklearn.estimator.sklearn` estimator, which accepts several constructor arguments:\n",
    "\n",
    "* __entry_point__: The path to the Python script SageMaker runs for training and prediction.\n",
    "* __role__: Role ARN\n",
    "* __framework_version__: Scikit-learn version you want to use for executing your model training code.\n",
    "* __train_instance_type__ *(optional)*: The type of SageMaker instances for training. __Note__: Because Scikit-learn does not natively support GPU training, Sagemaker Scikit-learn does not currently support training on GPU instance types.\n",
    "* __sagemaker_session__ *(optional)*: The session used to train on Sagemaker.\n",
    "\n",
    "To see the code for the SKLearn Estimator, see here: https://github.com/aws/sagemaker-python-sdk/tree/master/src/sagemaker/sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "\n",
    "FRAMEWORK_VERSION = \"0.23-1\"\n",
    "script_path = \"sklearn_abalone_featurizer.py\"\n",
    "\n",
    "sklearn_preprocessor = SKLearn(\n",
    "    entry_point=script_path,\n",
    "    role=role,\n",
    "    framework_version=FRAMEWORK_VERSION,\n",
    "    instance_type=\"ml.c4.xlarge\",\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2021-06-17 17:37:19 Starting - Starting the training job...\n",
      "2021-06-17 17:37:43 Starting - Launching requested ML instancesProfilerReport-1623951439: InProgress\n",
      "......\n",
      "2021-06-17 17:38:43 Starting - Preparing the instances for training.........\n",
      "2021-06-17 17:40:20 Downloading - Downloading input data\n",
      "2021-06-17 17:40:20 Training - Downloading the training image...\n",
      "2021-06-17 17:40:49 Uploading - Uploading generated training model.\u001b[34m2021-06-17 17:40:44,934 sagemaker-containers INFO     Imported framework sagemaker_sklearn_container.training\u001b[0m\n",
      "\u001b[34m2021-06-17 17:40:44,936 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-06-17 17:40:44,945 sagemaker_sklearn_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2021-06-17 17:40:45,321 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-06-17 17:40:45,334 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-06-17 17:40:45,347 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-06-17 17:40:45,358 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_sklearn_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"sagemaker-scikit-learn-2021-06-17-17-37-19-712\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-926521026587/sagemaker-scikit-learn-2021-06-17-17-37-19-712/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"sklearn_abalone_featurizer\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"sklearn_abalone_featurizer.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=sklearn_abalone_featurizer.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=sklearn_abalone_featurizer\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_sklearn_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-926521026587/sagemaker-scikit-learn-2021-06-17-17-37-19-712/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_sklearn_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"sagemaker-scikit-learn-2021-06-17-17-37-19-712\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-926521026587/sagemaker-scikit-learn-2021-06-17-17-37-19-712/source/sourcedir.tar.gz\",\"module_name\":\"sklearn_abalone_featurizer\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"sklearn_abalone_featurizer.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/miniconda3/bin:/miniconda3/lib/python37.zip:/miniconda3/lib/python3.7:/miniconda3/lib/python3.7/lib-dynload:/miniconda3/lib/python3.7/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python sklearn_abalone_featurizer.py\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34msaved model!\u001b[0m\n",
      "\u001b[34m2021-06-17 17:40:47,210 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2021-06-17 17:41:04 Completed - Training job completed\n",
      "Training seconds: 56\n",
      "Billable seconds: 56\n"
     ]
    }
   ],
   "source": [
    "sklearn_preprocessor.fit({\"train\": train_input})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch transform our training data <a class=\"anchor\" id=\"preprocess_train_data\"></a>\n",
    "Now that our proprocessor is properly fitted, let's go ahead and preprocess our training data. Let's use batch transform to directly preprocess the raw data and store right back into s3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a SKLearn Transformer from the trained SKLearn Estimator\n",
    "transformer = sklearn_preprocessor.transformer(\n",
    "    instance_count=1, instance_type=\"ml.m5.xlarge\", assemble_with=\"Line\", accept=\"text/csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ".............................\u001b[34m2021-06-17 17:48:49,234 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-06-17 17:48:49,236 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-06-17 17:48:49,237 INFO - sagemaker-containers - nginx config: \u001b[0m\n",
      "\u001b[34mworker_processes auto;\u001b[0m\n",
      "\u001b[34mdaemon off;\u001b[0m\n",
      "\u001b[34mpid /tmp/nginx.pid;\u001b[0m\n",
      "\u001b[35m2021-06-17 17:48:49,234 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2021-06-17 17:48:49,236 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2021-06-17 17:48:49,237 INFO - sagemaker-containers - nginx config: \u001b[0m\n",
      "\u001b[35mworker_processes auto;\u001b[0m\n",
      "\u001b[35mdaemon off;\u001b[0m\n",
      "\u001b[35mpid /tmp/nginx.pid;\u001b[0m\n",
      "\u001b[34merror_log  /dev/stderr;\n",
      "\u001b[0m\n",
      "\u001b[34mworker_rlimit_nofile 4096;\n",
      "\u001b[0m\n",
      "\u001b[34mevents {\n",
      "  worker_connections 2048;\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mhttp {\n",
      "  include /etc/nginx/mime.types;\n",
      "  default_type application/octet-stream;\n",
      "  access_log /dev/stdout combined;\n",
      "\n",
      "  upstream gunicorn {\n",
      "    server unix:/tmp/gunicorn.sock;\n",
      "  }\n",
      "\n",
      "  server {\n",
      "    listen 8080 deferred;\n",
      "    client_max_body_size 0;\n",
      "\n",
      "    keepalive_timeout 3;\n",
      "\n",
      "    location ~ ^/(ping|invocations|execution-parameters) {\n",
      "      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
      "      proxy_set_header Host $http_host;\n",
      "      proxy_redirect off;\n",
      "      proxy_read_timeout 60s;\n",
      "      proxy_pass http://gunicorn;\n",
      "    }\n",
      "\n",
      "    location / {\n",
      "      return 404 \"{}\";\n",
      "    }\n",
      "\n",
      "  }\u001b[0m\n",
      "\u001b[34m}\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m2021-06-17 17:48:49,376 INFO - sagemaker-containers - Module sklearn_abalone_featurizer does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2021-06-17 17:48:49,376 INFO - sagemaker-containers - Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2021-06-17 17:48:49,377 INFO - sagemaker-containers - Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2021-06-17 17:48:49,377 INFO - sagemaker-containers - Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python -m pip install . \u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[35merror_log  /dev/stderr;\n",
      "\u001b[0m\n",
      "\u001b[35mworker_rlimit_nofile 4096;\n",
      "\u001b[0m\n",
      "\u001b[35mevents {\n",
      "  worker_connections 2048;\u001b[0m\n",
      "\u001b[35m}\n",
      "\u001b[0m\n",
      "\u001b[35mhttp {\n",
      "  include /etc/nginx/mime.types;\n",
      "  default_type application/octet-stream;\n",
      "  access_log /dev/stdout combined;\n",
      "\n",
      "  upstream gunicorn {\n",
      "    server unix:/tmp/gunicorn.sock;\n",
      "  }\n",
      "\n",
      "  server {\n",
      "    listen 8080 deferred;\n",
      "    client_max_body_size 0;\n",
      "\n",
      "    keepalive_timeout 3;\n",
      "\n",
      "    location ~ ^/(ping|invocations|execution-parameters) {\n",
      "      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
      "      proxy_set_header Host $http_host;\n",
      "      proxy_redirect off;\n",
      "      proxy_read_timeout 60s;\n",
      "      proxy_pass http://gunicorn;\n",
      "    }\n",
      "\n",
      "    location / {\n",
      "      return 404 \"{}\";\n",
      "    }\n",
      "\n",
      "  }\u001b[0m\n",
      "\u001b[35m}\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[35m2021-06-17 17:48:49,376 INFO - sagemaker-containers - Module sklearn_abalone_featurizer does not provide a setup.py. \u001b[0m\n",
      "\u001b[35mGenerating setup.py\u001b[0m\n",
      "\u001b[35m2021-06-17 17:48:49,376 INFO - sagemaker-containers - Generating setup.cfg\u001b[0m\n",
      "\u001b[35m2021-06-17 17:48:49,377 INFO - sagemaker-containers - Generating MANIFEST.in\u001b[0m\n",
      "\u001b[35m2021-06-17 17:48:49,377 INFO - sagemaker-containers - Installing module with the following command:\u001b[0m\n",
      "\u001b[35m/miniconda3/bin/python -m pip install . \u001b[0m\n",
      "\u001b[35mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: sklearn-abalone-featurizer\n",
      "  Building wheel for sklearn-abalone-featurizer (setup.py): started\n",
      "  Building wheel for sklearn-abalone-featurizer (setup.py): finished with status 'done'\n",
      "  Created wheel for sklearn-abalone-featurizer: filename=sklearn_abalone_featurizer-1.0.0-py2.py3-none-any.whl size=8355 sha256=0ede53398419ba36dad71b72c8b6a3e841866eab2041c28acf4de8c31a16a865\n",
      "  Stored in directory: /home/model-server/tmp/pip-ephem-wheel-cache-rpp1vqmb/wheels/3e/0f/51/2f1df833dd0412c1bc2f5ee56baac195b5be563353d111dca6\u001b[0m\n",
      "\u001b[34mSuccessfully built sklearn-abalone-featurizer\u001b[0m\n",
      "\u001b[34mInstalling collected packages: sklearn-abalone-featurizer\u001b[0m\n",
      "\u001b[34mSuccessfully installed sklearn-abalone-featurizer-1.0.0\u001b[0m\n",
      "\u001b[34m[2021-06-17 17:48:51 +0000] [36] [INFO] Starting gunicorn 20.0.4\u001b[0m\n",
      "\u001b[34m[2021-06-17 17:48:51 +0000] [36] [INFO] Listening at: unix:/tmp/gunicorn.sock (36)\u001b[0m\n",
      "\u001b[34m[2021-06-17 17:48:51 +0000] [36] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[34m[2021-06-17 17:48:51 +0000] [39] [INFO] Booting worker with pid: 39\u001b[0m\n",
      "\u001b[34m[2021-06-17 17:48:51 +0000] [40] [INFO] Booting worker with pid: 40\u001b[0m\n",
      "\u001b[34m[2021-06-17 17:48:51 +0000] [41] [INFO] Booting worker with pid: 41\u001b[0m\n",
      "\u001b[34m[2021-06-17 17:48:51 +0000] [42] [INFO] Booting worker with pid: 42\u001b[0m\n",
      "\u001b[35mBuilding wheels for collected packages: sklearn-abalone-featurizer\n",
      "  Building wheel for sklearn-abalone-featurizer (setup.py): started\n",
      "  Building wheel for sklearn-abalone-featurizer (setup.py): finished with status 'done'\n",
      "  Created wheel for sklearn-abalone-featurizer: filename=sklearn_abalone_featurizer-1.0.0-py2.py3-none-any.whl size=8355 sha256=0ede53398419ba36dad71b72c8b6a3e841866eab2041c28acf4de8c31a16a865\n",
      "  Stored in directory: /home/model-server/tmp/pip-ephem-wheel-cache-rpp1vqmb/wheels/3e/0f/51/2f1df833dd0412c1bc2f5ee56baac195b5be563353d111dca6\u001b[0m\n",
      "\u001b[35mSuccessfully built sklearn-abalone-featurizer\u001b[0m\n",
      "\u001b[35mInstalling collected packages: sklearn-abalone-featurizer\u001b[0m\n",
      "\u001b[35mSuccessfully installed sklearn-abalone-featurizer-1.0.0\u001b[0m\n",
      "\u001b[35m[2021-06-17 17:48:51 +0000] [36] [INFO] Starting gunicorn 20.0.4\u001b[0m\n",
      "\u001b[35m[2021-06-17 17:48:51 +0000] [36] [INFO] Listening at: unix:/tmp/gunicorn.sock (36)\u001b[0m\n",
      "\u001b[35m[2021-06-17 17:48:51 +0000] [36] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[35m[2021-06-17 17:48:51 +0000] [39] [INFO] Booting worker with pid: 39\u001b[0m\n",
      "\u001b[35m[2021-06-17 17:48:51 +0000] [40] [INFO] Booting worker with pid: 40\u001b[0m\n",
      "\u001b[35m[2021-06-17 17:48:51 +0000] [41] [INFO] Booting worker with pid: 41\u001b[0m\n",
      "\u001b[35m[2021-06-17 17:48:51 +0000] [42] [INFO] Booting worker with pid: 42\u001b[0m\n",
      "\u001b[34m2021-06-17 17:48:53,805 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2021-06-17 17:48:53,805 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [17/Jun/2021:17:48:54 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [17/Jun/2021:17:48:54 +0000] \"GET /execution-parameters HTTP/1.1\" 404 232 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/compose/_column_transformer.py:440: FutureWarning: Given feature/column names or counts do not match the ones for the data given during fit. This will fail from v0.24.\n",
      "  FutureWarning)\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [17/Jun/2021:17:48:54 +0000] \"POST /invocations HTTP/1.1\" 200 641942 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [17/Jun/2021:17:48:54 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [17/Jun/2021:17:48:54 +0000] \"GET /execution-parameters HTTP/1.1\" 404 232 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.7/site-packages/sklearn/compose/_column_transformer.py:440: FutureWarning: Given feature/column names or counts do not match the ones for the data given during fit. This will fail from v0.24.\n",
      "  FutureWarning)\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [17/Jun/2021:17:48:54 +0000] \"POST /invocations HTTP/1.1\" 200 641942 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[32m2021-06-17T17:48:54.334:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\n",
      "Waiting for transform job: sagemaker-scikit-learn-2021-06-17-17-44-16-608\n",
      "\u001b[34m2021-06-17 17:48:49,234 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-06-17 17:48:49,236 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-06-17 17:48:49,237 INFO - sagemaker-containers - nginx config: \u001b[0m\n",
      "\u001b[34mworker_processes auto;\u001b[0m\n",
      "\u001b[34mdaemon off;\u001b[0m\n",
      "\u001b[34mpid /tmp/nginx.pid;\u001b[0m\n",
      "\u001b[35m2021-06-17 17:48:49,234 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2021-06-17 17:48:49,236 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2021-06-17 17:48:49,237 INFO - sagemaker-containers - nginx config: \u001b[0m\n",
      "\u001b[35mworker_processes auto;\u001b[0m\n",
      "\u001b[35mdaemon off;\u001b[0m\n",
      "\u001b[35mpid /tmp/nginx.pid;\u001b[0m\n",
      "\u001b[34merror_log  /dev/stderr;\n",
      "\u001b[0m\n",
      "\u001b[34mworker_rlimit_nofile 4096;\n",
      "\u001b[0m\n",
      "\u001b[34mevents {\n",
      "  worker_connections 2048;\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mhttp {\n",
      "  include /etc/nginx/mime.types;\n",
      "  default_type application/octet-stream;\n",
      "  access_log /dev/stdout combined;\n",
      "\n",
      "  upstream gunicorn {\n",
      "    server unix:/tmp/gunicorn.sock;\n",
      "  }\n",
      "\n",
      "  server {\n",
      "    listen 8080 deferred;\n",
      "    client_max_body_size 0;\n",
      "\n",
      "    keepalive_timeout 3;\n",
      "\n",
      "    location ~ ^/(ping|invocations|execution-parameters) {\n",
      "      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
      "      proxy_set_header Host $http_host;\n",
      "      proxy_redirect off;\n",
      "      proxy_read_timeout 60s;\n",
      "      proxy_pass http://gunicorn;\n",
      "    }\n",
      "\n",
      "    location / {\n",
      "      return 404 \"{}\";\n",
      "    }\n",
      "\n",
      "  }\u001b[0m\n",
      "\u001b[34m}\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m2021-06-17 17:48:49,376 INFO - sagemaker-containers - Module sklearn_abalone_featurizer does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2021-06-17 17:48:49,376 INFO - sagemaker-containers - Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2021-06-17 17:48:49,377 INFO - sagemaker-containers - Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2021-06-17 17:48:49,377 INFO - sagemaker-containers - Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python -m pip install . \u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[35merror_log  /dev/stderr;\n",
      "\u001b[0m\n",
      "\u001b[35mworker_rlimit_nofile 4096;\n",
      "\u001b[0m\n",
      "\u001b[35mevents {\n",
      "  worker_connections 2048;\u001b[0m\n",
      "\u001b[35m}\n",
      "\u001b[0m\n",
      "\u001b[35mhttp {\n",
      "  include /etc/nginx/mime.types;\n",
      "  default_type application/octet-stream;\n",
      "  access_log /dev/stdout combined;\n",
      "\n",
      "  upstream gunicorn {\n",
      "    server unix:/tmp/gunicorn.sock;\n",
      "  }\n",
      "\n",
      "  server {\n",
      "    listen 8080 deferred;\n",
      "    client_max_body_size 0;\n",
      "\n",
      "    keepalive_timeout 3;\n",
      "\n",
      "    location ~ ^/(ping|invocations|execution-parameters) {\n",
      "      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
      "      proxy_set_header Host $http_host;\n",
      "      proxy_redirect off;\n",
      "      proxy_read_timeout 60s;\n",
      "      proxy_pass http://gunicorn;\n",
      "    }\n",
      "\n",
      "    location / {\n",
      "      return 404 \"{}\";\n",
      "    }\n",
      "\n",
      "  }\u001b[0m\n",
      "\u001b[35m}\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[35m2021-06-17 17:48:49,376 INFO - sagemaker-containers - Module sklearn_abalone_featurizer does not provide a setup.py. \u001b[0m\n",
      "\u001b[35mGenerating setup.py\u001b[0m\n",
      "\u001b[35m2021-06-17 17:48:49,376 INFO - sagemaker-containers - Generating setup.cfg\u001b[0m\n",
      "\u001b[35m2021-06-17 17:48:49,377 INFO - sagemaker-containers - Generating MANIFEST.in\u001b[0m\n",
      "\u001b[35m2021-06-17 17:48:49,377 INFO - sagemaker-containers - Installing module with the following command:\u001b[0m\n",
      "\u001b[35m/miniconda3/bin/python -m pip install . \u001b[0m\n",
      "\u001b[35mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: sklearn-abalone-featurizer\n",
      "  Building wheel for sklearn-abalone-featurizer (setup.py): started\n",
      "  Building wheel for sklearn-abalone-featurizer (setup.py): finished with status 'done'\n",
      "  Created wheel for sklearn-abalone-featurizer: filename=sklearn_abalone_featurizer-1.0.0-py2.py3-none-any.whl size=8355 sha256=0ede53398419ba36dad71b72c8b6a3e841866eab2041c28acf4de8c31a16a865\n",
      "  Stored in directory: /home/model-server/tmp/pip-ephem-wheel-cache-rpp1vqmb/wheels/3e/0f/51/2f1df833dd0412c1bc2f5ee56baac195b5be563353d111dca6\u001b[0m\n",
      "\u001b[34mSuccessfully built sklearn-abalone-featurizer\u001b[0m\n",
      "\u001b[34mInstalling collected packages: sklearn-abalone-featurizer\u001b[0m\n",
      "\u001b[34mSuccessfully installed sklearn-abalone-featurizer-1.0.0\u001b[0m\n",
      "\u001b[34m[2021-06-17 17:48:51 +0000] [36] [INFO] Starting gunicorn 20.0.4\u001b[0m\n",
      "\u001b[34m[2021-06-17 17:48:51 +0000] [36] [INFO] Listening at: unix:/tmp/gunicorn.sock (36)\u001b[0m\n",
      "\u001b[34m[2021-06-17 17:48:51 +0000] [36] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[34m[2021-06-17 17:48:51 +0000] [39] [INFO] Booting worker with pid: 39\u001b[0m\n",
      "\u001b[34m[2021-06-17 17:48:51 +0000] [40] [INFO] Booting worker with pid: 40\u001b[0m\n",
      "\u001b[34m[2021-06-17 17:48:51 +0000] [41] [INFO] Booting worker with pid: 41\u001b[0m\n",
      "\u001b[34m[2021-06-17 17:48:51 +0000] [42] [INFO] Booting worker with pid: 42\u001b[0m\n",
      "\u001b[35mBuilding wheels for collected packages: sklearn-abalone-featurizer\n",
      "  Building wheel for sklearn-abalone-featurizer (setup.py): started\n",
      "  Building wheel for sklearn-abalone-featurizer (setup.py): finished with status 'done'\n",
      "  Created wheel for sklearn-abalone-featurizer: filename=sklearn_abalone_featurizer-1.0.0-py2.py3-none-any.whl size=8355 sha256=0ede53398419ba36dad71b72c8b6a3e841866eab2041c28acf4de8c31a16a865\n",
      "  Stored in directory: /home/model-server/tmp/pip-ephem-wheel-cache-rpp1vqmb/wheels/3e/0f/51/2f1df833dd0412c1bc2f5ee56baac195b5be563353d111dca6\u001b[0m\n",
      "\u001b[35mSuccessfully built sklearn-abalone-featurizer\u001b[0m\n",
      "\u001b[35mInstalling collected packages: sklearn-abalone-featurizer\u001b[0m\n",
      "\u001b[35mSuccessfully installed sklearn-abalone-featurizer-1.0.0\u001b[0m\n",
      "\u001b[35m[2021-06-17 17:48:51 +0000] [36] [INFO] Starting gunicorn 20.0.4\u001b[0m\n",
      "\u001b[35m[2021-06-17 17:48:51 +0000] [36] [INFO] Listening at: unix:/tmp/gunicorn.sock (36)\u001b[0m\n",
      "\u001b[35m[2021-06-17 17:48:51 +0000] [36] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[35m[2021-06-17 17:48:51 +0000] [39] [INFO] Booting worker with pid: 39\u001b[0m\n",
      "\u001b[35m[2021-06-17 17:48:51 +0000] [40] [INFO] Booting worker with pid: 40\u001b[0m\n",
      "\u001b[35m[2021-06-17 17:48:51 +0000] [41] [INFO] Booting worker with pid: 41\u001b[0m\n",
      "\u001b[35m[2021-06-17 17:48:51 +0000] [42] [INFO] Booting worker with pid: 42\u001b[0m\n",
      "\u001b[34m2021-06-17 17:48:53,805 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2021-06-17 17:48:53,805 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [17/Jun/2021:17:48:54 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [17/Jun/2021:17:48:54 +0000] \"GET /execution-parameters HTTP/1.1\" 404 232 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/compose/_column_transformer.py:440: FutureWarning: Given feature/column names or counts do not match the ones for the data given during fit. This will fail from v0.24.\n",
      "  FutureWarning)\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [17/Jun/2021:17:48:54 +0000] \"POST /invocations HTTP/1.1\" 200 641942 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [17/Jun/2021:17:48:54 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [17/Jun/2021:17:48:54 +0000] \"GET /execution-parameters HTTP/1.1\" 404 232 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.7/site-packages/sklearn/compose/_column_transformer.py:440: FutureWarning: Given feature/column names or counts do not match the ones for the data given during fit. This will fail from v0.24.\n",
      "  FutureWarning)\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [17/Jun/2021:17:48:54 +0000] \"POST /invocations HTTP/1.1\" 200 641942 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[32m2021-06-17T17:48:54.334:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Preprocess training input\n",
    "transformer.transform(train_input, content_type=\"text/csv\")\n",
    "print(\"Waiting for transform job: \" + transformer.latest_transform_job.job_name)\n",
    "transformer.wait()\n",
    "preprocessed_train = transformer.output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-926521026587/sagemaker-scikit-learn-2021-06-17-17-44-16-608'"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "preprocessed_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit a LinearLearner Model with the preprocessed data <a class=\"anchor\" id=\"training_model\"></a>\n",
    "Let's take the preprocessed training data and fit a LinearLearner Model. Sagemaker provides prebuilt algorithm containers that can be used with the Python SDK. The previous Scikit-learn job preprocessed the raw Titanic dataset into labeled, useable data that we can now use to fit a binary classifier Linear Learner model.\n",
    "\n",
    "For more on Linear Learner see: https://docs.aws.amazon.com/sagemaker/latest/dg/linear-learner.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.image_uris import retrieve\n",
    "\n",
    "ll_image = retrieve(\"linear-learner\", boto_session.region_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "293309674813197}}}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623952654.7730486, \"EndTime\": 1623952654.7730584, \"Dimensions\": {\"Algorithm\": \"Linear Learner\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 13, \"model\": 21}, \"Metrics\": {\"train_mse_objective\": {\"sum\": 0.944401360847629, \"count\": 1, \"min\": 0.944401360847629, \"max\": 0.944401360847629}}}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623952654.7730958, \"EndTime\": 1623952654.7731044, \"Dimensions\": {\"Algorithm\": \"Linear Learner\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 13, \"model\": 22}, \"Metrics\": {\"train_mse_objective\": {\"sum\": 0.8293406346669564, \"count\": 1, \"min\": 0.8293406346669564, \"max\": 0.8293406346669564}}}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623952654.7731347, \"EndTime\": 1623952654.7731426, \"Dimensions\": {\"Algorithm\": \"Linear Learner\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 13, \"model\": 23}, \"Metrics\": {\"train_mse_objective\": {\"sum\": 0.944387152332526, \"count\": 1, \"min\": 0.944387152332526, \"max\": 0.944387152332526}}}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623952654.773174, \"EndTime\": 1623952654.7731874, \"Dimensions\": {\"Algorithm\": \"Linear Learner\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 13, \"model\": 24}, \"Metrics\": {\"train_mse_objective\": {\"sum\": 0.9738491685344622, \"count\": 1, \"min\": 0.9738491685344622, \"max\": 0.9738491685344622}}}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623952654.7732291, \"EndTime\": 1623952654.773239, \"Dimensions\": {\"Algorithm\": \"Linear Learner\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 13, \"model\": 25}, \"Metrics\": {\"train_mse_objective\": {\"sum\": 1.022435462818696, \"count\": 1, \"min\": 1.022435462818696, \"max\": 1.022435462818696}}}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623952654.7732737, \"EndTime\": 1623952654.7732823, \"Dimensions\": {\"Algorithm\": \"Linear Learner\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 13, \"model\": 26}, \"Metrics\": {\"train_mse_objective\": {\"sum\": 0.9738801748133623, \"count\": 1, \"min\": 0.9738801748133623, \"max\": 0.9738801748133623}}}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623952654.7733574, \"EndTime\": 1623952654.7733738, \"Dimensions\": {\"Algorithm\": \"Linear Learner\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 13, \"model\": 27}, \"Metrics\": {\"train_mse_objective\": {\"sum\": 1.0224669481699282, \"count\": 1, \"min\": 1.0224669481699282, \"max\": 1.0224669481699282}}}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623952654.7734199, \"EndTime\": 1623952654.773429, \"Dimensions\": {\"Algorithm\": \"Linear Learner\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 13, \"model\": 28}, \"Metrics\": {\"train_mse_objective\": {\"sum\": 1.0862125550898223, \"count\": 1, \"min\": 1.0862125550898223, \"max\": 1.0862125550898223}}}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623952654.7734647, \"EndTime\": 1623952654.7734733, \"Dimensions\": {\"Algorithm\": \"Linear Learner\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 13, \"model\": 29}, \"Metrics\": {\"train_mse_objective\": {\"sum\": 1.1596406091291172, \"count\": 1, \"min\": 1.1596406091291172, \"max\": 1.1596406091291172}}}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623952654.7735088, \"EndTime\": 1623952654.7735221, \"Dimensions\": {\"Algorithm\": \"Linear Learner\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 13, \"model\": 30}, \"Metrics\": {\"train_mse_objective\": {\"sum\": 1.0862059912429407, \"count\": 1, \"min\": 1.0862059912429407, \"max\": 1.0862059912429407}}}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623952654.773576, \"EndTime\": 1623952654.7735863, \"Dimensions\": {\"Algorithm\": \"Linear Learner\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 13, \"model\": 31}, \"Metrics\": {\"train_mse_objective\": {\"sum\": 1.1595762658004578, \"count\": 1, \"min\": 1.1595762658004578, \"max\": 1.1595762658004578}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/17/2021 17:57:34 INFO 140492331382592] #quality_metric: host=algo-1, epoch=13, train mse_objective <loss>=0.5081583276104469\u001b[0m\n",
      "\u001b[34m[06/17/2021 17:57:34 INFO 140492331382592] #early_stopping_criteria_metric: host=algo-1, epoch=13, criteria=mse_objective, value=0.5081583276104469\u001b[0m\n",
      "\u001b[34m[06/17/2021 17:57:34 INFO 140492331382592] Epoch 13: Loss improved. Updating best model\u001b[0m\n",
      "\u001b[34m[06/17/2021 17:57:34 INFO 140492331382592] Saving model for epoch: 13\u001b[0m\n",
      "\u001b[34m[06/17/2021 17:57:34 INFO 140492331382592] Saved checkpoint to \"/tmp/tmp2uqcxljl/mx-mod-0000.params\"\u001b[0m\n",
      "\u001b[34m[06/17/2021 17:57:34 INFO 140492331382592] #progress_metric: host=algo-1, completed 93.33333333333333 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623952651.3454401, \"EndTime\": 1623952654.7837775, \"Dimensions\": {\"Algorithm\": \"Linear Learner\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 13, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 62687.0, \"count\": 1, \"min\": 62687, \"max\": 62687}, \"Total Batches Seen\": {\"sum\": 1966.0, \"count\": 1, \"min\": 1966, \"max\": 1966}, \"Max Records Seen Between Resets\": {\"sum\": 4177.0, \"count\": 1, \"min\": 4177, \"max\": 4177}, \"Max Batches Seen Between Resets\": {\"sum\": 131.0, \"count\": 1, \"min\": 131, \"max\": 131}, \"Reset Count\": {\"sum\": 16.0, \"count\": 1, \"min\": 16, \"max\": 16}, \"Number of Records Since Last Reset\": {\"sum\": 4177.0, \"count\": 1, \"min\": 4177, \"max\": 4177}, \"Number of Batches Since Last Reset\": {\"sum\": 131.0, \"count\": 1, \"min\": 131, \"max\": 131}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/17/2021 17:57:34 INFO 140492331382592] #throughput_metric: host=algo-1, train throughput=1214.777363004611 records/second\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623952658.1801844, \"EndTime\": 1623952658.180248, \"Dimensions\": {\"Algorithm\": \"Linear Learner\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 14, \"model\": 0}, \"Metrics\": {\"train_mse_objective\": {\"sum\": 0.5049522950672186, \"count\": 1, \"min\": 0.5049522950672186, \"max\": 0.5049522950672186}}}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623952658.180332, \"EndTime\": 1623952658.180348, \"Dimensions\": {\"Algorithm\": \"Linear Learner\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 14, \"model\": 1}, \"Metrics\": {\"train_mse_objective\": {\"sum\": 0.5272596850991249, \"count\": 1, \"min\": 0.5272596850991249, \"max\": 0.5272596850991249}}}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623952658.1803832, \"EndTime\": 1623952658.1803925, \"Dimensions\": {\"Algorithm\": \"Linear Learner\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 14, \"model\": 2}, \"Metrics\": {\"train_mse_objective\": {\"sum\": 0.5060801115746681, \"count\": 1, \"min\": 0.5060801115746681, \"max\": 0.5060801115746681}}}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623952658.1804473, \"EndTime\": 1623952658.1804585, \"Dimensions\": {\"Algorithm\": \"Linear Learner\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 14, \"model\": 3}, \"Metrics\": {\"train_mse_objective\": {\"sum\": 0.5281694063773522, \"count\": 1, \"min\": 0.5281694063773522, \"max\": 0.5281694063773522}}}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623952658.1804967, \"EndTime\": 1623952658.1805058, \"Dimensions\": {\"Algorithm\": \"Linear Learner\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 14, \"model\": 4}, \"Metrics\": {\"train_mse_objective\": {\"sum\": 0.6009375082185635, \"count\": 1, \"min\": 0.6009375082185635, \"max\": 0.6009375082185635}}}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623952658.1805384, \"EndTime\": 1623952658.180547, \"Dimensions\": {\"Algorithm\": \"Linear Learner\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 14, \"model\": 5}, \"Metrics\": {\"train_mse_objective\": {\"sum\": 0.8775278028960412, \"count\": 1, \"min\": 0.8775278028960412, \"max\": 0.8775278028960412}}}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623952658.1805997, \"EndTime\": 1623952658.1806147, \"Dimensions\": {\"Algorithm\": \"Linear Learner\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 14, \"model\": 6}, \"Metrics\": {\"train_mse_objective\": {\"sum\": 0.6022818881158646, \"count\": 1, \"min\": 0.6022818881158646, \"max\": 0.6022818881158646}}}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623952658.1806736, \"EndTime\": 1623952658.180689, \"Dimensions\": {\"Algorithm\": \"Linear Learner\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 14, \"model\": 7}, \"Metrics\": {\"train_mse_objective\": {\"sum\": 0.8775080198278794, \"count\": 1, \"min\": 0.8775080198278794, \"max\": 0.8775080198278794}}}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623952658.1807508, \"EndTime\": 1623952658.180767, \"Dimensions\": {\"Algorithm\": \"Linear Learner\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 14, \"model\": 8}, \"Metrics\": {\"train_mse_objective\": {\"sum\": 0.5063643662115702, \"count\": 1, \"min\": 0.5063643662115702, \"max\": 0.5063643662115702}}}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623952658.1808279, \"EndTime\": 1623952658.1808445, \"Dimensions\": {\"Algorithm\": \"Linear Learner\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 14, \"model\": 9}, \"Metrics\": {\"train_mse_objective\": {\"sum\": 0.5319390974365747, \"count\": 1, \"min\": 0.5319390974365747, \"max\": 0.5319390974365747}}}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623952658.180906, \"EndTime\": 1623952658.1809235, \"Dimensions\": {\"Algorithm\": \"Linear Learner\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 14, \"model\": 10}, \"Metrics\": {\"train_mse_objective\": {\"sum\": 0.5091646019369364, \"count\": 1, \"min\": 0.5091646019369364, \"max\": 0.5091646019369364}}}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623952658.1809847, \"EndTime\": 1623952658.1809967, \"Dimensions\": {\"Algorithm\": \"Linear Learner\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 14, \"model\": 11}, \"Metrics\": {\"train_mse_objective\": {\"sum\": 0.5320569424675061, \"count\": 1, \"min\": 0.5320569424675061, \"max\": 0.5320569424675061}}}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623952658.1810365, \"EndTime\": 1623952658.181046, \"Dimensions\": {\"Algorithm\": \"Linear Learner\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 14, \"model\": 12}, \"Metrics\": {\"train_mse_objective\": {\"sum\": 0.6102005976323898, \"count\": 1, \"min\": 0.6102005976323898, \"max\": 0.6102005976323898}}}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623952658.1811054, \"EndTime\": 1623952658.1811213, \"Dimensions\": {\"Algorithm\": \"Linear Learner\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 14, \"model\": 13}, \"Metrics\": {\"train_mse_objective\": {\"sum\": 0.880500814375969, \"count\": 1, \"min\": 0.880500814375969, \"max\": 0.880500814375969}}}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623952658.181181, \"EndTime\": 1623952658.1811972, \"Dimensions\": {\"Algorithm\": \"Linear Learner\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 14, \"model\": 14}, \"Metrics\": {\"train_mse_objective\": {\"sum\": 0.610218868805812, \"count\": 1, \"min\": 0.610218868805812, \"max\": 0.610218868805812}}}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623952658.181258, \"EndTime\": 1623952658.1812694, \"Dimensions\": {\"Algorithm\": \"Linear Learner\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 14, \"model\": 15}, \"Metrics\": {\"train_mse_objective\": {\"sum\": 0.8805011869049989, \"count\": 1, \"min\": 0.8805011869049989, \"max\": 0.8805011869049989}}}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623952658.1813488, \"EndTime\": 1623952658.1813653, \"Dimensions\": {\"Algorithm\": \"Linear Learner\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 14, \"model\": 16}, \"Metrics\": {\"train_mse_objective\": {\"sum\": 0.6447016176696007, \"count\": 1, \"min\": 0.6447016176696007, \"max\": 0.6447016176696007}}}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623952658.1814308, \"EndTime\": 1623952658.1814423, \"Dimensions\": {\"Algorithm\": \"Linear Learner\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 14, \"model\": 17}, \"Metrics\": {\"train_mse_objective\": {\"sum\": 0.7176205590940439, \"count\": 1, \"min\": 0.7176205590940439, \"max\": 0.7176205590940439}}}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623952658.1814764, \"EndTime\": 1623952658.1814902, \"Dimensions\": {\"Algorithm\": \"Linear Learner\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 14, \"model\": 18}, \"Metrics\": {\"train_mse_objective\": {\"sum\": 0.6447021656311476, \"count\": 1, \"min\": 0.6447021656311476, \"max\": 0.6447021656311476}}}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623952658.1815438, \"EndTime\": 1623952658.1815598, \"Dimensions\": {\"Algorithm\": \"Linear Learner\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 14, \"model\": 19}, \"Metrics\": {\"train_mse_objective\": {\"sum\": 0.7176042410043569, \"count\": 1, \"min\": 0.7176042410043569, \"max\": 0.7176042410043569}}}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623952658.181614, \"EndTime\": 1623952658.1816294, \"Dimensions\": {\"Algorithm\": \"Linear Learner\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 14, \"model\": 20}, \"Metrics\": {\"train_mse_objective\": {\"sum\": 0.8229262556594151, \"count\": 1, \"min\": 0.8229262556594151, \"max\": 0.8229262556594151}}}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623952658.1816843, \"EndTime\": 1623952658.1816998, \"Dimensions\": {\"Algorithm\": \"Linear Learner\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 14, \"model\": 21}, \"Metrics\": {\"train_mse_objective\": {\"sum\": 0.9339445374332942, \"count\": 1, \"min\": 0.9339445374332942, \"max\": 0.9339445374332942}}}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623952658.181751, \"EndTime\": 1623952658.1817667, \"Dimensions\": {\"Algorithm\": \"Linear Learner\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 14, \"model\": 22}, \"Metrics\": {\"train_mse_objective\": {\"sum\": 0.8229304714271656, \"count\": 1, \"min\": 0.8229304714271656, \"max\": 0.8229304714271656}}}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623952658.1818228, \"EndTime\": 1623952658.1818376, \"Dimensions\": {\"Algorithm\": \"Linear Learner\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 14, \"model\": 23}, \"Metrics\": {\"train_mse_objective\": {\"sum\": 0.9339337069827777, \"count\": 1, \"min\": 0.9339337069827777, \"max\": 0.9339337069827777}}}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623952658.1818895, \"EndTime\": 1623952658.1819046, \"Dimensions\": {\"Algorithm\": \"Linear Learner\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 14, \"model\": 24}, \"Metrics\": {\"train_mse_objective\": {\"sum\": 0.9719945097771975, \"count\": 1, \"min\": 0.9719945097771975, \"max\": 0.9719945097771975}}}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623952658.1819587, \"EndTime\": 1623952658.181974, \"Dimensions\": {\"Algorithm\": \"Linear Learner\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 14, \"model\": 25}, \"Metrics\": {\"train_mse_objective\": {\"sum\": 1.0217386517960292, \"count\": 1, \"min\": 1.0217386517960292, \"max\": 1.0217386517960292}}}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623952658.1820264, \"EndTime\": 1623952658.1820416, \"Dimensions\": {\"Algorithm\": \"Linear Learner\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 14, \"model\": 26}, \"Metrics\": {\"train_mse_objective\": {\"sum\": 0.9720265940404855, \"count\": 1, \"min\": 0.9720265940404855, \"max\": 0.9720265940404855}}}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623952658.1820965, \"EndTime\": 1623952658.182111, \"Dimensions\": {\"Algorithm\": \"Linear Learner\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 14, \"model\": 27}, \"Metrics\": {\"train_mse_objective\": {\"sum\": 1.021765866015966, \"count\": 1, \"min\": 1.021765866015966, \"max\": 1.021765866015966}}}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623952658.1821604, \"EndTime\": 1623952658.1821754, \"Dimensions\": {\"Algorithm\": \"Linear Learner\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 14, \"model\": 28}, \"Metrics\": {\"train_mse_objective\": {\"sum\": 1.084587069715445, \"count\": 1, \"min\": 1.084587069715445, \"max\": 1.084587069715445}}}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623952658.182219, \"EndTime\": 1623952658.182233, \"Dimensions\": {\"Algorithm\": \"Linear Learner\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 14, \"model\": 29}, \"Metrics\": {\"train_mse_objective\": {\"sum\": 1.1515506354662088, \"count\": 1, \"min\": 1.1515506354662088, \"max\": 1.1515506354662088}}}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623952658.182284, \"EndTime\": 1623952658.1822996, \"Dimensions\": {\"Algorithm\": \"Linear Learner\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 14, \"model\": 30}, \"Metrics\": {\"train_mse_objective\": {\"sum\": 1.0845785034963717, \"count\": 1, \"min\": 1.0845785034963717, \"max\": 1.0845785034963717}}}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623952658.182349, \"EndTime\": 1623952658.1823642, \"Dimensions\": {\"Algorithm\": \"Linear Learner\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 14, \"model\": 31}, \"Metrics\": {\"train_mse_objective\": {\"sum\": 1.1514988076228363, \"count\": 1, \"min\": 1.1514988076228363, \"max\": 1.1514988076228363}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/17/2021 17:57:38 INFO 140492331382592] #quality_metric: host=algo-1, epoch=14, train mse_objective <loss>=0.5049522950672186\u001b[0m\n",
      "\u001b[34m[06/17/2021 17:57:38 INFO 140492331382592] #early_stopping_criteria_metric: host=algo-1, epoch=14, criteria=mse_objective, value=0.5049522950672186\u001b[0m\n",
      "\u001b[34m[06/17/2021 17:57:38 INFO 140492331382592] Epoch 14: Loss improved. Updating best model\u001b[0m\n",
      "\u001b[34m[06/17/2021 17:57:38 INFO 140492331382592] Saving model for epoch: 14\u001b[0m\n",
      "\u001b[34m[06/17/2021 17:57:38 INFO 140492331382592] Saved checkpoint to \"/tmp/tmps1xb51q3/mx-mod-0000.params\"\u001b[0m\n",
      "\u001b[34m[06/17/2021 17:57:38 INFO 140492331382592] #progress_metric: host=algo-1, completed 100.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623952654.7840583, \"EndTime\": 1623952658.1925275, \"Dimensions\": {\"Algorithm\": \"Linear Learner\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 14, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 66864.0, \"count\": 1, \"min\": 66864, \"max\": 66864}, \"Total Batches Seen\": {\"sum\": 2097.0, \"count\": 1, \"min\": 2097, \"max\": 2097}, \"Max Records Seen Between Resets\": {\"sum\": 4177.0, \"count\": 1, \"min\": 4177, \"max\": 4177}, \"Max Batches Seen Between Resets\": {\"sum\": 131.0, \"count\": 1, \"min\": 131, \"max\": 131}, \"Reset Count\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Number of Records Since Last Reset\": {\"sum\": 4177.0, \"count\": 1, \"min\": 4177, \"max\": 4177}, \"Number of Batches Since Last Reset\": {\"sum\": 131.0, \"count\": 1, \"min\": 131, \"max\": 131}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/17/2021 17:57:38 INFO 140492331382592] #throughput_metric: host=algo-1, train throughput=1225.4217050963646 records/second\u001b[0m\n",
      "\u001b[34m[06/17/2021 17:57:38 WARNING 140492331382592] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[06/17/2021 17:57:38 WARNING 140492331382592] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[06/17/2021 17:57:38 INFO 140492331382592] #train_score (algo-1) : ('mse_objective', 5.1011368160416755)\u001b[0m\n",
      "\u001b[34m[06/17/2021 17:57:38 INFO 140492331382592] #train_score (algo-1) : ('mse', 5.1011368160416755)\u001b[0m\n",
      "\u001b[34m[06/17/2021 17:57:38 INFO 140492331382592] #train_score (algo-1) : ('absolute_loss', 1.5949838787624075)\u001b[0m\n",
      "\u001b[34m[06/17/2021 17:57:38 INFO 140492331382592] #quality_metric: host=algo-1, train mse_objective <loss>=5.1011368160416755\u001b[0m\n",
      "\u001b[34m[06/17/2021 17:57:38 INFO 140492331382592] #quality_metric: host=algo-1, train mse <loss>=5.1011368160416755\u001b[0m\n",
      "\u001b[34m[06/17/2021 17:57:38 INFO 140492331382592] #quality_metric: host=algo-1, train absolute_loss <loss>=1.5949838787624075\u001b[0m\n",
      "\u001b[34m[06/17/2021 17:57:38 INFO 140492331382592] Best model found for hyperparameters: {\"optimizer\": \"adam\", \"learning_rate\": 0.005, \"l1\": 0.0, \"wd\": 0.0001, \"lr_scheduler_step\": 10, \"lr_scheduler_factor\": 0.99, \"lr_scheduler_minimum_lr\": 1e-05}\u001b[0m\n",
      "\u001b[34m[06/17/2021 17:57:38 INFO 140492331382592] Saved checkpoint to \"/tmp/tmp99n2rmr9/mx-mod-0000.params\"\u001b[0m\n",
      "\u001b[34m[06/17/2021 17:57:38 INFO 140492331382592] Test data is not provided.\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623952604.905552, \"EndTime\": 1623952658.845037, \"Dimensions\": {\"Algorithm\": \"Linear Learner\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"initialize.time\": {\"sum\": 1600.825548171997, \"count\": 1, \"min\": 1600.825548171997, \"max\": 1600.825548171997}, \"epochs\": {\"sum\": 15.0, \"count\": 1, \"min\": 15, \"max\": 15}, \"check_early_stopping.time\": {\"sum\": 16.507387161254883, \"count\": 15, \"min\": 1.0056495666503906, \"max\": 1.2576580047607422}, \"update.time\": {\"sum\": 51632.057189941406, \"count\": 15, \"min\": 3200.9758949279785, \"max\": 3826.4989852905273}, \"finalize.time\": {\"sum\": 646.543025970459, \"count\": 1, \"min\": 646.543025970459, \"max\": 646.543025970459}, \"setuptime\": {\"sum\": 28.299331665039062, \"count\": 1, \"min\": 28.299331665039062, \"max\": 28.299331665039062}, \"totaltime\": {\"sum\": 54222.97430038452, \"count\": 1, \"min\": 54222.97430038452, \"max\": 54222.97430038452}}}\n",
      "\u001b[0m\n",
      "\n",
      "2021-06-17 17:57:48 Uploading - Uploading generated training model\n",
      "2021-06-17 17:57:48 Completed - Training job completed\n",
      "Training seconds: 98\n",
      "Billable seconds: 98\n"
     ]
    }
   ],
   "source": [
    "s3_ll_output_key_prefix = \"ll_training_output\"\n",
    "s3_ll_output_location = \"s3://{}/{}/{}/{}\".format(\n",
    "    bucket, prefix, s3_ll_output_key_prefix, \"ll_model\"\n",
    ")\n",
    "\n",
    "ll_estimator = sagemaker.estimator.Estimator(\n",
    "    ll_image,\n",
    "    role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m4.2xlarge\",\n",
    "    volume_size=20,\n",
    "    max_run=3600,\n",
    "    input_mode=\"File\",\n",
    "    output_path=s3_ll_output_location,\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")\n",
    "\n",
    "ll_estimator.set_hyperparameters(feature_dim=10, predictor_type=\"regressor\", mini_batch_size=32)\n",
    "\n",
    "ll_train_data = sagemaker.inputs.TrainingInput(\n",
    "    preprocessed_train,\n",
    "    distribution=\"FullyReplicated\",\n",
    "    content_type=\"text/csv\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    ")\n",
    "\n",
    "data_channels = {\"train\": ll_train_data}\n",
    "ll_estimator.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serial Inference Pipeline with Scikit preprocessor and Linear Learner <a class=\"anchor\" id=\"serial_inference\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the inference pipeline <a class=\"anchor\" id=\"pipeline_setup\"></a>\n",
    "Setting up a Machine Learning pipeline can be done with the Pipeline Model. This sets up a list of models in a single endpoint; in this example, we configure our pipeline model with the fitted Scikit-learn inference model and the fitted Linear Learner model. Deploying the model follows the same ```deploy``` pattern in the SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model import Model\n",
    "from sagemaker.pipeline import PipelineModel\n",
    "from time import gmtime, strftime\n",
    "\n",
    "timestamp_prefix = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "scikit_learn_inferencee_model = sklearn_preprocessor.create_model()\n",
    "linear_learner_model = ll_estimator.create_model()\n",
    "\n",
    "model_name = \"inference-pipeline-\" + timestamp_prefix\n",
    "endpoint_name = \"inference-pipeline-ep-\" + timestamp_prefix\n",
    "sm_model = PipelineModel(\n",
    "    name=model_name, role=role, models=[scikit_learn_inferencee_model, linear_learner_model]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Must setup local AWS configuration with a region supported by SageMaker.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-ee4ca5fc8380>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msm_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeploy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_instance_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ml.c4.xlarge\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendpoint_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mendpoint_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/sagemaker-practice/lib/python3.8/site-packages/sagemaker/pipeline.py\u001b[0m in \u001b[0;36mdeploy\u001b[0;34m(self, initial_instance_count, instance_type, serializer, deserializer, endpoint_name, tags, wait, update_endpoint, data_capture_config, kms_key)\u001b[0m\n\u001b[1;32m    168\u001b[0m         \"\"\"\n\u001b[1;32m    169\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0mcontainers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline_container_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sagemaker-practice/lib/python3.8/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, boto_session, sagemaker_client, sagemaker_runtime_client, sagemaker_featurestore_runtime_client, default_bucket)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         self._initialize(\n\u001b[0m\u001b[1;32m    121\u001b[0m             \u001b[0mboto_session\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mboto_session\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0msagemaker_client\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sagemaker-practice/lib/python3.8/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, boto_session, sagemaker_client, sagemaker_runtime_client, sagemaker_featurestore_runtime_client)\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_region_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboto_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregion_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_region_name\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    144\u001b[0m                 \u001b[0;34m\"Must setup local AWS configuration with a region supported by SageMaker.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             )\n",
      "\u001b[0;31mValueError\u001b[0m: Must setup local AWS configuration with a region supported by SageMaker."
     ]
    }
   ],
   "source": [
    "sm_model.deploy(initial_instance_count=1, instance_type=\"ml.c4.xlarge\", endpoint_name=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a request to our pipeline endpoint <a class=\"anchor\" id=\"pipeline_inference_request\"></a>\n",
    "\n",
    "Here we just grab the first line from the test data (you'll notice that the inference python script is very particular about the ordering of the inference request data). The ```ContentType``` field configures the first container, while the ```Accept``` field configures the last container. You can also specify each container's ```Accept``` and ```ContentType``` values using environment variables.\n",
    "\n",
    "We make our request with the payload in ```'text/csv'``` format, since that is what our script currently supports. If other formats need to be supported, this would have to be added to the ```output_fn()``` method in our entry point. Note that we set the ```Accept``` to ```application/json```, since Linear Learner does not support ```text/csv``` ```Accept```. The prediction output in this case is trying to guess the number of rings the abalone specimen would have given its other physical features; the actual number of rings is 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "\n",
    "payload = \"M, 0.44, 0.365, 0.125, 0.516, 0.2155, 0.114, 0.155\"\n",
    "actual_rings = 10\n",
    "predictor = Predictor(\n",
    "    endpoint_name=endpoint_name, sagemaker_session=sagemaker_session, serializer=CSVSerializer()\n",
    ")\n",
    "\n",
    "print(predictor.predict(payload))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete Endpoint <a class=\"anchor\" id=\"delete_endpoint\"></a>\n",
    "Once we are finished with the endpoint, we clean up the resources!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client = sagemaker_session.boto_session.client(\"sagemaker\")\n",
    "sm_client.delete_endpoint(EndpointName=endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "name": "python3",
   "display_name": "Python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "interpreter": {
   "hash": "66a809b2ad7d42417f5aa8d1e8fdf4c00bf17f2d930ede9ca680cd107750812c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}